{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "np.bool8 = np.bool\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "# import config\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "\n",
    "from torch.autograd import grad as torch_grad\n",
    "\n",
    "import h5py\n",
    "\n",
    "# import replay_buffer, learning_rate\n",
    "# from learning_rate import LearningRate\n",
    "\n",
    "import os\n",
    "os.environ['LD_LIBRARY_PATH'] = \"/home/garyding/.mujoco/mujoco210/bin:/usr/lib/nvidia\"\n",
    "os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRate:\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        lr (float)\n",
    "        decay_factor (float)\n",
    "        training_step (int)\n",
    "    \"\"\"\n",
    "    __instance = None\n",
    "\n",
    "    def __init__(self):\n",
    "        if LearningRate.__instance is not None:\n",
    "            raise Exception(\"Singleton instantiation called twice\")\n",
    "        else:\n",
    "            LearningRate.__instance = self\n",
    "            self.lr = None\n",
    "            self.decay_factor = None\n",
    "            self.training_step = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_instance():\n",
    "        \"\"\"Get the singleton instance.\n",
    "\n",
    "        Returns:\n",
    "            (LearningRate)\n",
    "        \"\"\"\n",
    "        if LearningRate.__instance is None:\n",
    "            LearningRate()\n",
    "        return LearningRate.__instance\n",
    "\n",
    "    def set_learning_rate(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def get_learning_rate(self):\n",
    "        return self.lr\n",
    "\n",
    "    def increment_step(self):\n",
    "        self.training_step += 1\n",
    "\n",
    "    def get_step(self):\n",
    "        return self.training_step\n",
    "\n",
    "    def set_decay(self, d):\n",
    "        self.decay_factor = d\n",
    "\n",
    "    def decay(self):\n",
    "        if self.lr is None:\n",
    "            raise ValueError(\"Learning rate has not been set.\")\n",
    "        self.lr = self.lr * self.decay_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple replay buffer\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, env):\n",
    "        self.buffer = []  # No size limitation, similar to the paper\n",
    "        self.zeroAction = np.zeros_like(env.action_space.sample(), dtype=np.float32)\n",
    "        self.absorbingState = np.zeros((env.observation_space.shape[0]), dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    # data = (state, action, next_state)\n",
    "    def add(self, data, done):\n",
    "        if done:\n",
    "            self.buffer.append((data[0], data[1], self.absorbingState, False))\n",
    "        else:\n",
    "            self.buffer.append((data[0], data[1], data[2], False))\n",
    "\n",
    "    def addAbsorbing(self):\n",
    "        self.buffer.append((self.absorbingState, self.zeroAction, self.absorbingState, False))\n",
    "\n",
    "    def sample(self, batch_size=100):\n",
    "        ind = np.random.randint(0, len(self.buffer), size=batch_size)\n",
    "        s, a, ns, d = [], [], [], []\n",
    "\n",
    "        for i in ind:\n",
    "            S, A, nS, D = self.buffer[i]\n",
    "            s.append(np.array(S))\n",
    "            a.append(np.array(A))\n",
    "            ns.append(np.array(nS))\n",
    "            d.append(np.array(D))\n",
    "\n",
    "        return np.array(s), np.array(a), np.array(ns), np.array(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy_weight = 0.001 from openAI/imiation\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_size=100, lamb=10, entropy_weight=0.001):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            num_inputs:\n",
    "            hidden_size:\n",
    "            lamb:\n",
    "            entropy_weight:\n",
    "            \n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, 1)\n",
    "        self.linear3.weight.data.mul_(0.1)\n",
    "        self.linear3.bias.data.mul_(0.0)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.entropy_weight = entropy_weight\n",
    "        self.optimizer = torch.optim.Adam(self.parameters())\n",
    "        self.LAMBDA = lamb  # used in gradient penalty\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        \n",
    "        self.loss = self.ce_loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if not self.use_cuda: x = x.float()\n",
    "        x = torch.tanh(self.linear1(x))\n",
    "        x = torch.tanh(self.linear2(x))\n",
    "        # prob = torch.sigmoid(self.linear3(x))\n",
    "        # return prob\n",
    "        out = self.linear3(x)\n",
    "        return out\n",
    "\n",
    "    def reward(self, x):\n",
    "        out = self(x)\n",
    "        probs = torch.sigmoid(out)\n",
    "        return torch.log(probs + 1e-8) - torch.log(1 - probs + 1e-8)\n",
    "\n",
    "    def adjust_adversary_learning_rate(self, lr):\n",
    "        print(\"Setting adversary learning rate to: {}\".format(lr))\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def logit_bernoulli_entropy(self, logits):\n",
    "        ent = (1. - torch.sigmoid(logits)) * logits - self.logsigmoid(logits)\n",
    "        return ent\n",
    "\n",
    "    def logsigmoid(self, a):\n",
    "        return torch.log(torch.sigmoid(a))\n",
    "\n",
    "    def logsigmoidminus(self, a):\n",
    "        return torch.log(1 - torch.sigmoid(a))\n",
    "\n",
    "    def ce_loss(self, pred_on_learner, pred_on_expert, expert_weights):\n",
    "        \"\"\"Binary cross entropy loss.\n",
    "        We believe this is the loss function the authors to communicate.\n",
    "\n",
    "        Args:\n",
    "            pred_on_learner (torch.Tensor): The discriminator's prediction on the learner.\n",
    "            pred_on_expert (torch.Tensor): The discriminator's prediction on the expert.\n",
    "            expert_weights (torch.Tensor): The weighting to apply to the expert loss\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor)\n",
    "        \"\"\"\n",
    "        learner_loss = torch.log(1 - torch.sigmoid(pred_on_learner))\n",
    "        expert_loss = torch.log(torch.sigmoid(pred_on_expert)) * expert_weights\n",
    "        return -torch.sum(learner_loss + expert_loss)\n",
    "\n",
    "    def learn(self, replay_buf, expert_buf, iterations, batch_size=100):\n",
    "        self.adjust_adversary_learning_rate(LearningRate.get_instance().lr)\n",
    "\n",
    "        for it in range(iterations):\n",
    "            # Sample replay buffer\n",
    "            x, y, u, d = replay_buf.sample(batch_size)\n",
    "            state = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "            action = torch.tensor(y, dtype=torch.float32, device=device)\n",
    "            next_state = torch.tensor(u, dtype=torch.float32, device=device)\n",
    "\n",
    "            # Sample expert buffer\n",
    "            expert_obs, expert_act, expert_weights = expert_buf.get_next_batch(batch_size)\n",
    "            expert_obs = torch.tensor(expert_obs, dtype=torch.float32, device=device)\n",
    "            expert_act = torch.tensor(expert_act, dtype=torch.float32, device=device)\n",
    "            expert_weights = torch.tensor(expert_weights, dtype=torch.float32, device=device).view(-1, 1)\n",
    "\n",
    "            # Predict\n",
    "            state_action = torch.cat([state, action], 1).to(device)\n",
    "            expert_state_action = torch.cat([expert_obs, expert_act], 1).to(device)\n",
    "\n",
    "            fake = self(state_action)\n",
    "            real = self(expert_state_action)\n",
    "\n",
    "            # Gradient penalty for regularization.\n",
    "            gradient_penalty = self._gradient_penalty(expert_state_action, state_action)\n",
    "\n",
    "            # The main discriminator loss\n",
    "            main_loss = self.loss(fake, real, expert_weights)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            total_loss = main_loss + gradient_penalty\n",
    "\n",
    "            if it == 0 or it == iterations - 1:\n",
    "                print(\"Discr Iteration:  {:03} ---- Loss: {:.5f} | Learner Prob: {:.5f} | Expert Prob: {:.5f}\".format(\n",
    "                    it, total_loss.item(), torch.sigmoid(fake[0]).item(), torch.sigmoid(real[0]).item()\n",
    "                ))\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def _gradient_penalty(self, real_data, generated_data):\n",
    "        \"\"\"\n",
    "        Compute the gradient penalty for the current update.\n",
    "        \"\"\"\n",
    "        batch_size = real_data.size(0)\n",
    "        device = real_data.device  # Get the device (CPU or GPU)\n",
    "\n",
    "        # Calculate interpolation\n",
    "        alpha = torch.rand(batch_size, 1, device=device)  # Move alpha to the same device as real_data\n",
    "        interpolated = alpha * real_data + (1 - alpha) * generated_data\n",
    "\n",
    "        # Ensure gradients are calculated for the interpolated data\n",
    "        interpolated.requires_grad_(True)\n",
    "\n",
    "        # Calculate probability of interpolated examples\n",
    "        prob_interpolated = self(interpolated)  # Forward pass through the discriminator\n",
    "\n",
    "        # Calculate gradients of probabilities with respect to examples\n",
    "        gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                        grad_outputs=torch.ones_like(prob_interpolated),\n",
    "                        create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "        # Flatten gradients to compute the L2 norm per example in the batch\n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "\n",
    "        # Compute gradient penalty\n",
    "        gradients_norm = gradients.norm(2, dim=1)  # L2 norm of the gradients\n",
    "        gradient_penalty = ((gradients_norm - 1) ** 2).mean()  # (||grad||_2 - 1)^2\n",
    "\n",
    "        return self.LAMBDA * gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    '''\n",
    "    \\pi(a|s): Given a sequence of states, return a sequence of actions\n",
    "    '''\n",
    "    def __init__(self, state_dim: int, action_dim: int, max_action: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        device = x.device\n",
    "        x = x.to(device).float()\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        x = torch.tanh(self.l3(x)) * self.max_action\n",
    "        return x\n",
    "\n",
    "    def act(self, x: Tensor) -> Tensor:\n",
    "        x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "        return self(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    '''\n",
    "    Given a squence of (state,action) pairs, return (Q1,Q2)\n",
    "    '''\n",
    "    def __init__(self, state_dim: int, action_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Q1 network architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "\n",
    "        # Q2 network architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l5 = nn.Linear(400, 300)\n",
    "        self.l6 = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        # Concatenate the state (x) and action (y)\n",
    "        xy = torch.cat([x, y], dim=1)\n",
    "\n",
    "        # Q1 computation\n",
    "        x1 = torch.relu(self.l1(xy))\n",
    "        x1 = torch.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "\n",
    "        # Q2 computation\n",
    "        x2 = torch.relu(self.l4(xy))\n",
    "        x2 = torch.relu(self.l5(x2))\n",
    "        x2 = self.l6(x2)\n",
    "\n",
    "        return x1, x2\n",
    "\n",
    "    def Q1(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        # Compute Q1 value alone (without Q2)\n",
    "        xy = torch.cat([x, y], dim=1)\n",
    "        x1 = torch.relu(self.l1(xy))\n",
    "        x1 = torch.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "        return x1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(object):\n",
    "    def __init__(self, state_dim, action_dim, max_action, actor_clipping, decay_steps):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "\n",
    "        self.decay_steps = decay_steps\n",
    "        self.actor_grad_clipping = actor_clipping\n",
    "        self.max_action = max_action\n",
    "        self.actor_steps = 0\n",
    "        self.critic_steps = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state.reshape(1, -1), dtype=torch.float32).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def adjust_critic_learning_rate(self, lr):\n",
    "        print(\"Setting critic learning rate to: {}\".format(lr))\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr)\n",
    "\n",
    "    def adjust_actor_learning_rate(self, lr):\n",
    "        print(\"Setting actor learning rate to: {}\".format(lr))\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
    "\n",
    "    def reward(self, discriminator, states, actions):\n",
    "        states_actions = torch.cat([states, actions], 1).to(device)\n",
    "        return discriminator.reward(states_actions)\n",
    "\n",
    "    def train(self, discriminator, replay_buf, iterations, batch_size=100, discount=0.8, tau=0.005, policy_noise=0.2,\n",
    "              noise_clip=0.5, policy_freq=2):\n",
    "\n",
    "        lr_tracker = LearningRate.get_instance()\n",
    "        lr = lr_tracker.lr\n",
    "\n",
    "        self.adjust_actor_learning_rate(lr)\n",
    "        self.adjust_critic_learning_rate(lr)\n",
    "\n",
    "        for iteration in range(iterations):\n",
    "            # Sample replay buffer\n",
    "            x, y, u, d = replay_buf.sample(batch_size)\n",
    "            # print(x.shape, y.shape, u.shape, d.shape)\n",
    "            state = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "            action = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "            next_state = torch.tensor(u, dtype=torch.float32).to(device)\n",
    "            reward = self.reward(discriminator, state, action)\n",
    "            \n",
    "            # Select action according to policy and add clipped noise\n",
    "            # Generate clipped noise\n",
    "            noise = torch.randn_like(action) * policy_noise  \n",
    "            noise = noise.clamp(-noise_clip, noise_clip)          \n",
    "\n",
    "            # Add noise to the action selected by the target actor network\n",
    "            # print('1: ',self.actor_target(next_state).shape)\n",
    "            # print('2: ',noise.shape)\n",
    "            # print('3: ',(self.actor_target(next_state)+noise).shape)\n",
    "            next_action = self.actor_target(next_state) + noise\n",
    "            # Clamp the action to the valid action space defined by the max action\n",
    "            next_action = next_action.clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            # print(next_state.shape, next_action.shape)\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            # target_Q = reward + (done * discount * target_Q).detach()\n",
    "            target_Q = reward + (discount * target_Q).detach()\n",
    "\n",
    "            # Get current Q estimates\n",
    "            current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "            if iteration == 0 or iteration == iterations - 1:\n",
    "                print(\"Critic Iteration: {:3} ---- Loss: {:.5f}\".format(iteration, critic_loss.item()))\n",
    "            # Optimize the critic\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Delayed policy updates\n",
    "            if iteration % policy_freq == 0:\n",
    "\n",
    "                # Compute actor loss\n",
    "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "                if iteration == 0 or iteration == iterations - 1 or iteration == iterations - 2:\n",
    "                    print(\"Actor Iteration:  {:3} ---- Loss: {:.5f}\".format(iteration, actor_loss.item()))\n",
    "                # Optimize the actor\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "\n",
    "                # Clip, like the paper\n",
    "                clip_grad_value_(self.actor.parameters(), self.actor_grad_clipping)\n",
    "\n",
    "                self.actor_optimizer.step()\n",
    "                lr_tracker.training_step += 1\n",
    "                step = lr_tracker.training_step\n",
    "\n",
    "                if step != 0 and step % self.decay_steps == 0:\n",
    "                    print(\"Decaying learning rate at step: {}\".format(step))\n",
    "                    lr_tracker.decay()\n",
    "\n",
    "                    self.adjust_actor_learning_rate(lr_tracker.lr)\n",
    "                    self.adjust_critic_learning_rate(lr_tracker.lr)\n",
    "\n",
    "                # Update the frozen target models\n",
    "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "    def load(self, filename, directory):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(evaluations, number_of_timesteps):\n",
    "    \"\"\"Store the results of a run.\n",
    "\n",
    "    Args:\n",
    "        evaluations:\n",
    "        number_of_timesteps (int):\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame.from_records(evaluations)\n",
    "    number_of_trajectories = len(evaluations[0]) - 1\n",
    "    columns = [\"reward_{}\".format(i) for i in range(number_of_trajectories)]\n",
    "    columns.append(\"timestep\")\n",
    "    df.columns = columns\n",
    "\n",
    "    timestamp = time.time()\n",
    "    results_fname = 'DAC_{}_tsteps_{}_results.csv'.format( number_of_timesteps, timestamp)\n",
    "    df.to_csv(str(results_fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs policy for X episodes and returns average reward\n",
    "def evaluate_policy(env, policy, time_step, evaluation_trajectories=6):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        env: The environment being trained on.\n",
    "        policy:\tThe policy being evaluated\n",
    "        time_step (int): The number of time steps the policy has been trained for.\n",
    "        evaluation_trajectories (int): The number of trajectories on which to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        (list)\t- The time_step, followed by all the rewards.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(evaluation_trajectories):\n",
    "        r = 0\n",
    "        obs = env.reset()[0]\n",
    "        done, trun = False,False\n",
    "        while (not done)&(not trun):\n",
    "            action = policy.select_action(np.array(obs))\n",
    "            obs, reward, done, trun, _ = env.step(action)\n",
    "            r += reward\n",
    "        rewards.append(r)\n",
    "    print(\"Average reward at timestep {}: {}\".format(time_step, np.mean(rewards)))\n",
    "\n",
    "    rewards.append(time_step)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_length = 1000\n",
    "batch_size = 1000\n",
    "num_steps = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LearningRate.get_instance()\n",
    "lr.lr = 10 ** (-3)\n",
    "lr.decay_factor = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, limit_trajs, data_subsamp_freq=1):\n",
    "    tmp = torch.load(path)\n",
    "    full_dset_size = tmp['state'].size(0)\n",
    "    dset_size = min(full_dset_size, limit_trajs) if limit_trajs is not None else full_dset_size\n",
    "\n",
    "    states = tmp['state'].reshape(100, full_dset_size//100, 3).clone()[:dset_size]\n",
    "    actions = tmp['action'].reshape(100, full_dset_size//100, 1).clone()[:dset_size]\n",
    "    rewards = tmp['reward'].reshape(100, full_dset_size//100, 1).clone()[:dset_size]\n",
    "    dones = tmp['done'].reshape(100, full_dset_size//100, 1).clone()[:dset_size]\n",
    "    next_states = tmp['next_state'].clone()[:dset_size]\n",
    "    return states, actions, rewards\n",
    "\n",
    "class Dset(object):\n",
    "    def __init__(self, obs, acs, num_traj, absorbing_state, absorbing_action):\n",
    "        self.obs = obs\n",
    "        self.acs = acs\n",
    "        self.num_traj = num_traj\n",
    "        assert len(self.obs) == len(self.acs)\n",
    "        assert self.num_traj > 0\n",
    "        self.steps_per_traj = int(len(self.obs) / num_traj)\n",
    "\n",
    "        self.absorbing_state = absorbing_state\n",
    "        self.absorbing_action = absorbing_action\n",
    "\n",
    "    def get_next_batch(self, batch_size):\n",
    "        assert batch_size <= len(self.obs)\n",
    "        num_samples_per_traj = int(batch_size / self.num_traj)\n",
    "        # print('mul', num_samples_per_traj * self.num_traj)\n",
    "        assert num_samples_per_traj * self.num_traj == batch_size\n",
    "        N = self.steps_per_traj / num_samples_per_traj  # This is the importance weight for\n",
    "        j = num_samples_per_traj\n",
    "        num_samples_per_traj = num_samples_per_traj - 1  # make room for absorbing\n",
    "\n",
    "        obs = None\n",
    "        acs = None\n",
    "        weights = [1 for i in range(batch_size)]\n",
    "        while j <= batch_size:\n",
    "            weights[j - 1] = 1.0 / N\n",
    "            j = j + num_samples_per_traj + 1\n",
    "        \n",
    "        # print(self.num_traj)\n",
    "        for i in range(self.num_traj):\n",
    "            indicies = np.sort(\n",
    "                np.random.choice(range(self.steps_per_traj * i, self.steps_per_traj * (i + 1)), num_samples_per_traj,\n",
    "                                 replace=False))\n",
    "            if obs is None:\n",
    "                obs = np.concatenate((self.obs[indicies, :], self.absorbing_state), axis=0)\n",
    "                \n",
    "            else:\n",
    "                obs = np.concatenate((obs, self.obs[indicies, :], self.absorbing_state), axis=0)\n",
    "\n",
    "            if acs is None:\n",
    "                acs = np.concatenate((self.acs[indicies, :], self.absorbing_action), axis=0)\n",
    "            else:\n",
    "                acs = np.concatenate((acs, self.acs[indicies, :], self.absorbing_action), axis=0)\n",
    "\n",
    "        return obs, acs, weights\n",
    "\n",
    "class Mujoco_Dset(object):\n",
    "    def __init__(self, env, expert_path, traj_limitation=-1):\n",
    "        obs, acs, rets = load_dataset(expert_path, traj_limitation)\n",
    "        # obs, acs: shape (N, L, ) + S where N = # episodes, L = episode length\n",
    "        # and S is the environment observation/action space.\n",
    "        # Flatten to (N * L, prod(S))\n",
    "        self.obs = np.reshape(obs, [-1, np.prod(obs.shape[2:])])\n",
    "        # self.acs = acs\n",
    "        self.acs = np.reshape(acs, [-1, np.prod(acs.shape[2:])])\n",
    "\n",
    "        self.rets = rets.sum(axis=1)\n",
    "        self.avg_ret = sum(self.rets) / len(self.rets)\n",
    "        self.std_ret = np.std(np.array(self.rets))\n",
    "        # if len(self.acs) > 2:\n",
    "        #     self.acs = np.squeeze(self.acs)\n",
    "        assert len(self.obs) == len(self.acs)\n",
    "        self.num_traj = len(rets)\n",
    "        self.num_transition = len(self.obs)\n",
    "\n",
    "        absorbing_state = np.zeros((1, env.observation_space.shape[0]), dtype=np.float32)\n",
    "        zero_action = np.zeros_like(env.action_space.sample(), dtype=np.float32).reshape(1, env.action_space.shape[0])\n",
    "        self.dset = Dset(self.obs, self.acs, traj_limitation, absorbing_state, zero_action)\n",
    "        self.log_info()\n",
    "\n",
    "    def log_info(self):\n",
    "        print(\"Total trajs: %d\" % self.num_traj)\n",
    "        print(\"Total transitions: %d\" % self.num_transition)\n",
    "        print(\"Average returns: %f\" % self.avg_ret)\n",
    "        print(\"Std for returns: %f\" % self.std_ret)\n",
    "\n",
    "    def get_next_batch(self, batch_size):\n",
    "        return self.dset.get_next_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trajs: 100\n",
      "Total transitions: 1000000\n",
      "Average returns: -12447.127930\n",
      "Std for returns: 1190.231812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3841760/445698455.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tmp = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "expert_buffer = Mujoco_Dset(env, 'size1000000_std0.01_prand0.0.pth', 100)\n",
    "actor_replay_buffer = ReplayBuffer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward at timestep 0: -1610.6053088019205\n"
     ]
    }
   ],
   "source": [
    "# TD3(state_dim, action_dim, max_action, actor_clipping, decay_steps*) *Not used yet;\n",
    "td3_policy = TD3(state_dim, action_dim, max_action, 40, 10 ** 5)\n",
    "\n",
    "# Input dim = state_dim + action_dim\n",
    "discriminator = Discriminator(state_dim + action_dim).to(device)\n",
    "\n",
    "# For storing temporary evaluations\n",
    "evaluations = [evaluate_policy(env, td3_policy, 0)]\n",
    "evaluate_every = 1000\n",
    "steps_since_eval = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.94295955,  0.33290738,  0.7437855 ], dtype=float32), {})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current step: 0\n",
      "Setting adversary learning rate to: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/garyding/anaconda3/envs/rlenv/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discr Iteration:  000 ---- Loss: 1325.66638 | Learner Prob: 0.49773 | Expert Prob: 0.49822\n",
      "Discr Iteration:  999 ---- Loss: 159.31540 | Learner Prob: 0.40968 | Expert Prob: 0.97465\n",
      "Setting actor learning rate to: 0.001\n",
      "Setting critic learning rate to: 0.001\n",
      "Critic Iteration:   0 ---- Loss: 24.54868\n",
      "Actor Iteration:    0 ---- Loss: 0.52470\n",
      "Actor Iteration:  998 ---- Loss: 3.50349\n",
      "Critic Iteration: 999 ---- Loss: 0.02586\n",
      "\n",
      "Current step: 1000\n",
      "Setting adversary learning rate to: 0.001\n",
      "Discr Iteration:  000 ---- Loss: 1604.98340 | Learner Prob: 0.07602 | Expert Prob: 0.97440\n",
      "Discr Iteration:  999 ---- Loss: 122.26205 | Learner Prob: 0.01162 | Expert Prob: 0.98059\n",
      "Setting actor learning rate to: 0.001\n",
      "Setting critic learning rate to: 0.001\n",
      "Critic Iteration:   0 ---- Loss: 10.65042\n",
      "Actor Iteration:    0 ---- Loss: 4.52143\n",
      "Actor Iteration:  998 ---- Loss: 11.95334\n",
      "Critic Iteration: 999 ---- Loss: 0.00546\n",
      "Average reward at timestep 2000: -1211.6257788527869\n",
      "\n",
      "Current step: 2000\n",
      "Setting adversary learning rate to: 0.001\n",
      "Discr Iteration:  000 ---- Loss: 343.27237 | Learner Prob: 0.05814 | Expert Prob: 0.98221\n",
      "Discr Iteration:  999 ---- Loss: 109.75014 | Learner Prob: 0.01395 | Expert Prob: 0.99146\n",
      "Setting actor learning rate to: 0.001\n",
      "Setting critic learning rate to: 0.001\n",
      "Critic Iteration:   0 ---- Loss: 1.54056\n",
      "Actor Iteration:    0 ---- Loss: 12.30340\n",
      "Actor Iteration:  998 ---- Loss: 4.46849\n",
      "Critic Iteration: 999 ---- Loss: 0.02892\n",
      "Average reward at timestep 3000: -1500.593056461492\n",
      "\n",
      "Current step: 3000\n",
      "Setting adversary learning rate to: 0.001\n",
      "Discr Iteration:  000 ---- Loss: 1451.79102 | Learner Prob: 0.02989 | Expert Prob: 0.98492\n",
      "Discr Iteration:  999 ---- Loss: 113.47472 | Learner Prob: 0.04451 | Expert Prob: 0.98552\n",
      "Setting actor learning rate to: 0.001\n",
      "Setting critic learning rate to: 0.001\n",
      "Critic Iteration:   0 ---- Loss: 30.45049\n",
      "Actor Iteration:    0 ---- Loss: 5.51522\n",
      "Actor Iteration:  998 ---- Loss: 10.68685\n",
      "Critic Iteration: 999 ---- Loss: 0.02872\n",
      "Average reward at timestep 4000: -921.81403697637\n",
      "\n",
      "Current step: 4000\n",
      "Setting adversary learning rate to: 0.001\n",
      "Discr Iteration:  000 ---- Loss: 853.70581 | Learner Prob: 0.02606 | Expert Prob: 0.99341\n",
      "Discr Iteration:  999 ---- Loss: 160.43958 | Learner Prob: 0.03078 | Expert Prob: 0.98503\n",
      "Setting actor learning rate to: 0.001\n",
      "Setting critic learning rate to: 0.001\n",
      "Critic Iteration:   0 ---- Loss: 4.26318\n",
      "Actor Iteration:    0 ---- Loss: 10.26543\n",
      "Actor Iteration:  998 ---- Loss: 10.05087\n",
      "Critic Iteration: 999 ---- Loss: 0.02443\n",
      "Average reward at timestep 5000: -1157.8658849976734\n",
      "\n",
      "Current step: 5000\n",
      "Setting adversary learning rate to: 0.001\n",
      "Discr Iteration:  000 ---- Loss: 173.58299 | Learner Prob: 0.03012 | Expert Prob: 0.98544\n",
      "Discr Iteration:  999 ---- Loss: 152.00772 | Learner Prob: 0.04803 | Expert Prob: 0.98427\n",
      "Setting actor learning rate to: 0.001\n",
      "Setting critic learning rate to: 0.001\n",
      "Critic Iteration:   0 ---- Loss: 0.18465\n",
      "Actor Iteration:    0 ---- Loss: 12.07857\n",
      "Actor Iteration:  998 ---- Loss: 10.81703\n",
      "Critic Iteration: 999 ---- Loss: 0.03570\n",
      "Average reward at timestep 6000: -1125.9421024587737\n",
      "\n",
      "Current step: 6000\n",
      "Setting adversary learning rate to: 0.001\n",
      "Discr Iteration:  000 ---- Loss: 603.72864 | Learner Prob: 0.01825 | Expert Prob: 0.93420\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# print((current_state.shape, action.shape, next_state.shape))\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         current_state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m---> 17\u001b[0m \u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactor_replay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpert_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrajectory_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m td3_policy\u001b[38;5;241m.\u001b[39mtrain(discriminator, actor_replay_buffer, trajectory_length, batch_size)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps_since_eval \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m evaluate_every:\n",
      "Cell \u001b[0;32mIn[4], line 83\u001b[0m, in \u001b[0;36mDiscriminator.learn\u001b[0;34m(self, replay_buf, expert_buf, iterations, batch_size)\u001b[0m\n\u001b[1;32m     80\u001b[0m next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(u, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Sample expert buffer\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m expert_obs, expert_act, expert_weights \u001b[38;5;241m=\u001b[39m \u001b[43mexpert_buf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m expert_obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(expert_obs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     85\u001b[0m expert_act \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(expert_act, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[14], line 90\u001b[0m, in \u001b[0;36mMujoco_Dset.get_next_batch\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_next_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size):\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 44\u001b[0m, in \u001b[0;36mDset.get_next_batch\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# print(self.num_traj)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_traj):\n\u001b[1;32m     43\u001b[0m     indicies \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msort(\n\u001b[0;32m---> 44\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_traj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_traj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples_per_traj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs[indicies, :], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabsorbing_state), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:960\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while len(actor_replay_buffer) < num_steps:\n",
    "    print(\"\\nCurrent step: {}\".format(len(actor_replay_buffer.buffer)))\n",
    "    current_state = env.reset()[0]\n",
    "    # Sample from policy; maybe we don't reset the environment -> since this may bias the policy toward initial observations\n",
    "    for j in range(trajectory_length):\n",
    "        action = td3_policy.select_action(np.array(current_state))\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            actor_replay_buffer.addAbsorbing()\n",
    "            current_state = env.reset()\n",
    "        else:\n",
    "            actor_replay_buffer.add((current_state, action, next_state), done)\n",
    "            # print((current_state.shape, action.shape, next_state.shape))\n",
    "            current_state = next_state\n",
    "\n",
    "    discriminator.learn(actor_replay_buffer, expert_buffer, trajectory_length, batch_size)\n",
    "\n",
    "    td3_policy.train(discriminator, actor_replay_buffer, trajectory_length, batch_size)\n",
    "\n",
    "    if steps_since_eval >= evaluate_every:\n",
    "        steps_since_eval = 0\n",
    "\n",
    "        evaluation = evaluate_policy(env, td3_policy, len(actor_replay_buffer))\n",
    "        evaluations.append(evaluation)\n",
    "\n",
    "    steps_since_eval += trajectory_length\n",
    "    \n",
    "last_evaluation = evaluate_policy(env, td3_policy, len(actor_replay_buffer))\n",
    "evaluations.append(last_evaluation)\n",
    "\n",
    "store_results(evaluations, len(actor_replay_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
